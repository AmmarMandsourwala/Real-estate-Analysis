{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties_raw_data = pd.read_csv('realproperties.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties_raw_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_raw_data = pd.read_csv('realcustomer.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_raw_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new variable that replicates the original data is crucial. \n",
    "\n",
    "properties = properties_raw_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# properties.describe() method allows us to examine statistical summaries of the data.\n",
    "# However, by default, it only includes numeric columns. \n",
    "# To include all variables, we specify 'include=all' in the parameter. \n",
    "\n",
    "properties.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since 'ID' serves as an identifier rather than a numeric variable, it's best to change its data type.\n",
    "# By converting it to a string, we ensure no numerical operations can be inadvertently performed on it. \n",
    "properties['id'] = properties['id'].astype(str)\n",
    "# Let's examine the 'id' column to confirm our changes.\n",
    "properties['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The same applies for the 'building' and 'property#' variables\n",
    "properties['building'] = properties['building'].astype(str)\n",
    "properties['property#'] = properties['property#'].astype(str)\n",
    "\n",
    "properties.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focusing on 'date of sale', we notice several issues:\n",
    "# Among 267 entries, there are 44 unique ones, which is unusual and warrants a closer look.\n",
    "\n",
    "properties['date_sale'].unique()\n",
    "\n",
    "# Our investigation reveals multiple problems:\n",
    "\n",
    "# 1. Some entries are '#NUM!', a placeholder for missing or erroneous values, likely carried over from an Excel file.\n",
    "\n",
    "# 2. Every date is listed as the first of the month, regardless of the actual month. \n",
    "# This suggests that our data may only include information about the month of the transaction, rather than the precise date.\n",
    "\n",
    "# 3. The 'date_sale' Series is currently classified as an 'object' type, \n",
    "# meaning the dates are treated as strings instead of actual dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's address these issues step by step.\n",
    "# The first task is to replace the Excel-specific missing value indicator '#NUM!' with a format \n",
    "# that pandas can understand as a null value, i.e., 'pd.NA'.\n",
    "properties['date_sale'] = np.where(properties['date_sale']=='#NUM!', pd.NA, properties['date_sale'])\n",
    "\n",
    "properties['date_sale'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next step is to convert the 'date_sale' column to a date type. \n",
    "properties['date_sale'] = pd.to_datetime(properties['date_sale'])\n",
    "properties['date_sale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties.describe(include=\"all\", datetime_is_numeric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties['type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's turn our attention to the 'type' column. \n",
    "# Although it appears to be well-formatted, we'll standardize it further by converting all its entries to lowercase.\n",
    "properties['type'] = properties['type'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "properties.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties.dtypes\n",
    "\n",
    "# interestingly, price is also of object type and we definitely want it to be a float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The '$' symbol in the 'price' column is causing it to be treated as a string. \n",
    "# To fix this, we'll strip the '$' sign and create a new Series 'price$'.\n",
    "properties['price$'] = properties['price'].str.strip('$')\n",
    "properties.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we intend to convert the 'price$' column to a float data type.\n",
    "#properties['price$'] = properties['price$'].astype(float)\n",
    "\n",
    "# Unfortunately, we encounter an error because of the thousands separator ',' still present in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the .strip() method only removes characters from the beginning and end of a string, \n",
    "# The .replace() method is apt for this task, replacing all instances of ',' with an empty string.\n",
    "properties['price$']= properties['price$'].replace(\",\",\"\",regex=True) \n",
    "\n",
    "properties.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can carry on with casting price into a float\n",
    "properties['price$'] = properties['price$'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = properties.drop(['price'],axis=1)\n",
    "properties.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties['status'].unique()\n",
    "\n",
    "# There don't appear to be any missing values.\n",
    "# However, we notice some inconsistencies: the word 'sold' is surrounded by spaces and is capitalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties['status'] = properties['status'].str.strip()\n",
    "properties['status'] = properties['status'].str.lower()\n",
    "\n",
    "properties['status'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = properties.rename(columns= {'status':'sold'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties['sold'] = properties['sold'].map({'sold':1,'-':0})\n",
    "properties['sold'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties.isna().sum()\n",
    "\n",
    "# From the initial glance, it seems there are no missing values, apart from the 'date_sale' column. \n",
    "# To be certain, you can verify this by inspecting each column with the .unique() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final property dataset\n",
    "properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = customers_raw_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "customers.columns.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename customerid in an appropriate way\n",
    "customers = customers.rename(columns= {'\\ufeffcustomerid':'customerid'})\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get an overview of our data.\n",
    "customers.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = customers.rename(columns= {'entity':'individual'})\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers['individual'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make 'individual' a Boolean-like variable (at least conceptually), let's map 'Individual' to 1 and 'Company' to 0.\n",
    "customers['individual'] = customers['individual'].map({'Individual':1,'Company':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers['sex'].unique()\n",
    "\n",
    "# It appears there are three options: 'F', 'M', and an empty string. \n",
    "# We should map the empty string to NaN to indicate missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to keep 'sex' as an 'object' variable, so we'll map 'F' and 'M' to '1' and '0' respectively. \n",
    "customers['sex'] = customers['sex'].map({'F':'1','M':'0', '':pd.NA})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers['purpose'] = customers['purpose'].str.lower()\n",
    "customers['source'] = customers['source'].str.lower()\n",
    "\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers['mortgage'] = customers['mortgage'].map({'No':0,'Yes':1})\n",
    "\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to create a new column 'full_name' that combines the 'name' and 'surname' of each customer.\n",
    "customers['full_name'] = customers['name'] + \" \" + customers['surname']\n",
    "\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's drop name and surname columns from the dataframe.\n",
    "customers = customers.drop(['name', 'surname'], axis=1)\n",
    "\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers['birth_date'] = pd.to_datetime(customers['birth_date'])\n",
    "\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the two tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the 'merge' function in pandas to combine the two dataframes.\n",
    "# The 'customerid' column is designated as the common key on which the dataframes will be merged. \n",
    "# We opt for a left join, retaining all rows from the 'properties' dataframe and appending matching rows from the 'customers' dataframe. \n",
    "# If there's no match, the resultant dataframe will have 'NA' for the corresponding 'customers' dataframe columns.\n",
    "pd.merge(properties, customers, on='customerid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The merge did not work as intended.\n",
    "properties.customerid.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's apparent that the two datasets differ - there are unexpected and unnecessary spaces.\n",
    "customers.customerid.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's trim the unnecessary white space from 'customerid' in the properties dataframe.\n",
    "properties['customerid'] = properties['customerid'].str.strip()\n",
    "customers['customerid'] = customers['customerid'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To ensure a successful merge, we must ensure that 'customerid' in the customers table has only unique values.\n",
    "customers.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties['customerid'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We've observed that there's an empty space in 'customerid' in the 'properties' dataframe.\n",
    "# Let's replace it with 'NA'. This corresponds to properties that are not yet purchased.\n",
    "properties['customerid'] = np.where(properties['customerid']=='', pd.NA, properties['customerid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's rename our merged dataframe to 'real_estate_data' for clarity.\n",
    "real_estate_data = pd.merge(properties, customers, on='customerid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_estate_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_estate_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_estate_data.fillna(pd.NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_estate_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll assign the cleaned and preprocessed data to a new variable 'data'.\n",
    "data = real_estate_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include=\"all\", datetime_is_numeric = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakdowns by building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine:\n",
    "1. Breakdown of totals by building (frequency distribution by building)\n",
    "2. Breakdown of averages by building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['building'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('building').sum()\n",
    "\n",
    "# it would be more sensible to select a subset of 'data' to aggregate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakdown of totals by building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_of_interest = ['building', 'sold','mortgage']\n",
    "totals_by_building = data[columns_of_interest].groupby(\"building\").sum()\n",
    "totals_by_building "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakdown of averages by building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_of_interest = ['building', 'area', 'price$', 'deal_satisfaction']\n",
    "averages_by_building = data[columns_of_interest].groupby(\"building\").mean()\n",
    "averages_by_building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakdowns by country and state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Country\n",
    "1. Breakdown of totals by country (frequency distribution by country)\n",
    "2. Breakdown of averages by country\n",
    "\n",
    "State\n",
    "3. Frequency distribution by state\n",
    "4. Relative frequency by state\n",
    "5. Cumulative frequency by state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakdown of totals by country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_of_interest = ['country', 'sold','mortgage']\n",
    "totals_by_country = data[columns_of_interest].groupby(\"country\").sum()\n",
    "totals_by_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's inspect the unique values in the 'country' column.\n",
    "data['country'].unique()\n",
    "\n",
    "# Clearly, there are duplicate entries for USA which needs to be addressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "object_columns = data.select_dtypes(['object']).columns\n",
    "object_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using this, we select only the data from these columns and\n",
    "# use the .apply() method to strip all white spaces from them simultaneously. \n",
    "data[object_columns] = data[object_columns].apply(lambda x: x.str.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals_by_country = data[columns_of_interest].groupby(\"country\").sum()\n",
    "totals_by_country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakdown of averages by country\n",
    "Based on what you have seen before for the breakdowns by bulding and by state, please find the breakdown by country of the columns 'area', 'deal_satisfaction', and 'price$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the average computations, we use the following list of columns.\n",
    "columns_of_interest = ['country', 'area', 'deal_satisfaction','price$']\n",
    "averages_by_country = data[columns_of_interest].groupby(\"country\").mean()\n",
    "averages_by_country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency distribution by state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_of_interest = ['state', 'sold','mortgage']\n",
    "totals_by_state = data[columns_of_interest].groupby(\"state\").sum()\n",
    "totals_by_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals_by_state.sold.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals_by_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['state'] = np.where(data['state']=='', pd.NA, data['state'])\n",
    "data['state'] = np.where(data['country']!='USA', pd.NA, data['state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals_by_state = data[columns_of_interest].groupby(\"state\").sum()\n",
    "totals_by_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals_by_state.sold.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To focus on the relative and cumulative frequency of sales, we can refine our table by state.\n",
    "columns_of_interest = ['state', 'sold']\n",
    "sold_by_state = data[columns_of_interest].groupby(\"state\").sum()\n",
    "sold_by_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get a clearer picture, we can sort the values in descending order.\n",
    "sold_by_state = sold_by_state.sort_values('sold', ascending=False)\n",
    "sold_by_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The term 'sold' may not be the best to describe frequency, so we can rename this column.\n",
    "sold_by_state = sold_by_state.rename(columns={'sold':'frequency'})\n",
    "sold_by_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative frequency distribution by state\n",
    "1. Add a new column to the 'sold_by_state' data frame, called 'relative_frequency' which contains the relative frequency of the different states.\n",
    "2. Add a new column to the 'sold_by_state' data frame, called 'cumulative_frequency' which contains the cumulative frequency of the different states. You can use your own tools to achieve this, or look up the .cumsum() method here: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.cumsum.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The relative frequency can be computed by dividing the frequency of each state by the total frequency.\n",
    "sold_by_state['relative_frequency'] = sold_by_state['frequency']/sold_by_state['frequency'].sum()\n",
    "sold_by_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative frequency can be obtained using the 'cumsum()' function in pandas.\n",
    "sold_by_state['cumulative_frequency'] = sold_by_state['relative_frequency'].cumsum()\n",
    "sold_by_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customers Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the age of our buyers at the time of purchase.\n",
    "# This can be done by subtracting the birth date from the sale date.\n",
    "data['age_at_purchase'] = data['date_sale'] - data['birth_date']\n",
    "data['age_at_purchase']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# timedelta has different attributes than normal dates, which you can explore in the docs\n",
    "# for our purposes, we want the age in integers, so we can work with it\n",
    "# to access the number of days stored inside, we can use the attribute 'days'\n",
    "\n",
    "#data['age_at_purchase'][0].days\n",
    "type(data['age_at_purchase'][0].days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# It's important to note that the 'age_at_purchase' field we've just created is of timedelta datatype.\n",
    "# A timedelta object represents a duration, the difference between two dates or times.\n",
    "# For our analysis, we need the age in integers or floats for better manipulation.\n",
    "# To convert it, we use the 'days' attribute to extract the number of days, as timedelta measures duration in days.\n",
    "\n",
    "data['age_at_purchase'] = data['age_at_purchase'].apply(lambda x: x.days)\n",
    "data['age_at_purchase']\n",
    "\n",
    "# Notice that the datatype is float64, which may be due to how pandas internally manages timedelta objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To obtain the age in years at the time of purchase, we can divide the 'age_at_purchase' field by 365.\n",
    "data['age_at_purchase'] = data['age_at_purchase']/365\n",
    "data['age_at_purchase']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since age is typically represented in whole numbers, we can round down the values using np.floor().\n",
    "data['age_at_purchase_rounded'] = data['age_at_purchase'].apply(lambda x: np.floor(x))\n",
    "data['age_at_purchase_rounded']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create age intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To divide age into different intervals, we can use the 'cut' method from pandas.\n",
    "# This function segments and sorts the data values into bins we specify.\n",
    "# Here, we're dividing the 'age_at_purchase' into 10 bins and setting the decimal precision to 0.\n",
    "data['age_interval'] = pd.cut(data['age_at_purchase'], bins = 10, precision = 0)\n",
    "data['age_interval']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakdown by age interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns_of_interest = ['age_interval', 'sold']\n",
    "sold_by_age = data[columns_of_interest].groupby(\"age_interval\").sum()\n",
    "sold_by_age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the price of properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['price_interval'] = pd.cut(data['price$'], bins=10)\n",
    "data['price_interval']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total number of properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_of_interest = ['price_interval', 'sold']\n",
    "all_properties_by_price = data[columns_of_interest].groupby(\"price_interval\").count()\n",
    "\n",
    "all_properties_by_price = all_properties_by_price.rename(columns={'sold':'count'})\n",
    "all_properties_by_price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total number of sold properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_of_interest = ['price_interval', 'sold']\n",
    "sold_properties_by_price = data[columns_of_interest].groupby(\"price_interval\").sum()\n",
    "sold_properties_by_price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total number of not sold properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To identify properties that remain unsold, we can subtract the sold properties from the total count.\n",
    "all_properties_by_price['not_sold'] = all_properties_by_price['count'] - sold_properties_by_price['sold']\n",
    "all_properties_by_price['sold'] = sold_properties_by_price['sold']\n",
    "all_properties_by_price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship between age and price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out only the sold apartments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sold = data[data['sold']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further, let's exclude any company data, focusing only on individual sales.\n",
    "data_sold = data_sold[data_sold['individual']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sold[['age_at_purchase','price$']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance of age and price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.cov(data_sold['age_at_purchase'], data_sold['price$'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation of age and price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(data_sold['age_at_purchase'], data_sold['price$'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As an alternative, we can use the correlation method provided by pandas, which gives the same result.\n",
    "data_sold_no_na[['age_at_purchase','price$']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deal Satisfaction Across Countries (Bar Chart)\n",
    "\n",
    "1. To plot the deal satisfaction by country, we must first obtain the relevant data. There needs to be a breakdown of deal satisfaction by country. If you remember, we have done that ealier.\n",
    "2. With the data we can create a bar chart with the following parameters: \n",
    "    - white background with a grid\n",
    "    - size of the figure (12,6)\n",
    "    - fitting color which is not the default one\n",
    "    - rotated x ticks, with a font size of 13\n",
    "    - y ticks, with a font size of 13\n",
    "    - y label for the deal satisfaction (no need for an x label)\n",
    "    - remove the top and the right border of the chart\n",
    "    - save the newly created figure as a .png on your computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will maintain these visualization parameters for all subsequent charts in order to ensure consistency and comparability across the different visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have already calculate the averages by country so we can take advantage of that\n",
    "averages_by_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\") \n",
    "\n",
    "plt.figure(figsize = (12, 6)) \n",
    "\n",
    "plt.bar(x = averages_by_country.index,height = averages_by_country['deal_satisfaction'],color = \"#108A99\") \n",
    "\n",
    "plt.xticks(rotation = 45, fontsize = 13) \n",
    "plt.yticks(fontsize = 13) \n",
    "plt.title(\"Deal Satisfaction by Country\", fontsize = 18, fontweight = \"bold\") \n",
    "plt.ylabel(\"Deal Satisfaction\", fontsize = 13 ) \n",
    "\n",
    "sns.despine() # removes the top and right border of our graph\n",
    "\n",
    "plt.savefig(\"deal_satisfaction_by_country_bar_chart.png\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Customer Age Distribution (Histogram) (Histogram)\n",
    "\n",
    "1. Ð¢o visualize the distribution of customer ages at the time of purchase, we'll generate a histogram. The necessary data for this visualization is already included in the data variable.\n",
    "2. Histogram construction: We use the <strong>data</strong> variable to create a histogram that represents the age distribution at purchase.The histogram should incorporate the following parameters:\n",
    "    - white background with a grid\n",
    "    - size of the figure (12,6)\n",
    "    - 10 bins for the different categories\n",
    "    - fitting color which is not the default one\n",
    "    - appropriate x and y labels\n",
    "    - remove the top and the right border of the chart\n",
    "    - save the newly created figure as a .png on your computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\") \n",
    "\n",
    "plt.figure(figsize = (12, 6)) \n",
    "plt.hist(data['age_at_purchase'],bins = 10,color = \"#108A99\")\n",
    "plt.title(\"Age Distribution\", fontsize = 18, weight = \"bold\")\n",
    "plt.xlabel(\"Age\", fontsize=13)\n",
    "plt.ylabel(\"Number of Purchases\", fontsize=13)\n",
    "\n",
    "sns.despine()\n",
    "plt.savefig(\"age_distribution_histogram.png\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Sales per Year (Line chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to create a line chart detailing total sales per year.\n",
    "data['date_sale'][0].year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do that for the whole series and save the year in a separate column\n",
    "data['year_sale'] = data['date_sale'].apply(lambda x: x.year)\n",
    "data['year_sale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to convert the 'year_sale' variable type from float to int as fractional years don't accurately represent our data.\n",
    "# To modify the data type, we employ the 'astype' method. Since this function cannot handle missing values, we initially replace all NAs with zeros.\n",
    "data['year_sale'] = data['year_sale'].fillna(0).astype(int)\n",
    "\n",
    "# We also need to convert all zeros back to NA, restoring our dataset's missing value representation.\n",
    "data['year_sale'] = data['year_sale'].replace({0:pd.NA})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we formulate the desired sales breakdown by year.\n",
    "columns_of_interest = ['year_sale','price$']\n",
    "revenue_per_year = data[columns_of_interest].groupby('year_sale').sum()\n",
    "\n",
    "revenue_per_year = revenue_per_year.rename(columns={'price$':'revenue$'})\n",
    "revenue_per_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize = (12, 6))\n",
    "\n",
    "plt.plot(revenue_per_year['revenue$'],color='#108A99',linewidth=3)\n",
    "\n",
    "plt.title(\"Total Revenue per Year (2004-2010)\", fontsize = 18, fontweight = \"bold\")\n",
    "plt.ylabel(\"Revenue $\", fontsize = 13)\n",
    "plt.xticks(fontsize = 13) \n",
    "plt.yticks(fontsize = 13) \n",
    "\n",
    "sns.despine() # We'll remove the top and right borders of the chart for a cleaner look.\n",
    "plt.savefig(\"total_revenue_per_year_in_M_line_chart.png\")\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Several issues detract from the effectiveness of our graph:\n",
    "# 1. Revenue figures are displayed in scientific notation, which is not readily interpretable.\n",
    "# 2. The year 2009 lacks its own tick mark, due to its absence from the revenue_per_year dataframe.\n",
    "\n",
    "# Currently, revenue is represented in dollars, but the figures are large and overwhelming.\n",
    "# For a cleaner, more comprehensible visualization, we should present revenue in thousands or millions.\n",
    "revenue_per_year['revenue$inM'] = revenue_per_year['revenue$'] / 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue_per_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To improve our data representation, we'll add a new data point for the year 2009.\n",
    "# This year will display 0 revenue, reflecting the actual situation.\n",
    "# This inclusion aids in presenting a more accurate reality.\n",
    "revenue_per_year_adj = revenue_per_year.copy() \n",
    "\n",
    "revenue_per_year_adj = revenue_per_year_adj.append({'revenue$': 0,'revenue$inM': 0},ignore_index=True)\n",
    "\n",
    "revenue_per_year_adj.index = ['2004','2005','2006','2007','2008','2010','2009']\n",
    "# Reordering the dataframe to chronologically arrange the years.\n",
    "revenue_per_year_adj = revenue_per_year_adj.loc[['2004','2005','2006','2007','2008','2009','2010']]\n",
    "revenue_per_year_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\") \n",
    "\n",
    "plt.figure(figsize = (12, 6)) \n",
    "\n",
    "plt.plot(revenue_per_year_adj['revenue$inM'],color='#108A99',linewidth=3)\n",
    "\n",
    "plt.title(\"Total Revenue per Year (2004-2010)\", fontsize = 18, fontweight = \"bold\")\n",
    "plt.ylabel(\"Revenue $ in Millions\", fontsize = 14)\n",
    "plt.xticks(fontsize = 13) \n",
    "plt.yticks(fontsize = 13) \n",
    "\n",
    "sns.despine() \n",
    "plt.savefig(\"total_revenue_per_year_in_M_line_chart.png\") \n",
    "plt.show() # Display the chart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yearly Sales Distribution Across Buildings (Stacked Area Chart)\n",
    "We're going to create a stacked area chart that displays the distribution of yearly sales across various buildings. Follow these step-by-step instructions:\n",
    "1. <strong>Initialize a checkpoint dataframe:</strong> Establish a new variable named <strong>data_stacked_area</strong> which will be our checkpoint for further computations.\n",
    "2.  <strong>Formulate building indicators:</strong> Construct indicator (or dummy) variables rooted in <strong>building</strong>. These can be stored directly in data_stacked_area. An alternate approach involves two steps: creating a separate <strong>building_dummies</strong> variable, and then appending it to <strong>data_stacked_area</strong>.\n",
    "3. <strong>Redefine column names:</strong> Amend the names of the dummy variables to be more descriptive. Simple labels such as 'building1', 'building2', etc. should work well.\n",
    "4. <strong>Exclude unsold properties:</strong> Remove all properties from the dataset that haven't been sold.\n",
    "5. <strong>Generate a yearly breakdown:</strong> Develop a breakdown by year for the 5 building dummy variables. This will yield a yearly sales distribution per building.\n",
    "6. <strong>Create a stacked area chart:</strong> With the data prepared, it's time to create the stacked area chart: \n",
    "    - Background: Opt for a white background with a grid.\n",
    "    - Figure size: Set the dimensions as (12,6).\n",
    "    - Color scheme: Use a 5-color scheme, with colors that distinctly represent the 5 buildings. \n",
    "    - Edgecolor: This should be none.\n",
    "    - Legend: Include a legend that maps the colors to their corresponding labels.\n",
    "    - Axes labels: Assign an x label and y label, both with a font size of 13.\n",
    "    - Chart borders: Discard the top and right borders of the chart.\n",
    "    - Save the figure: Store the final chart as a .png file on your local machine\n",
    "\n",
    "One point to keep in mind, as we noted during the Line Chart creation, the year 2009 isn't present in the data frame we are plotting. Hence, its corresponding value (0) isn't marked. To address this, generate a new stacked area chart that includes the year 2009 as an observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stacked_area = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_dummies = pd.get_dummies(data_stacked_area['building'])\n",
    "building_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_stacked_area = pd.concat([data_stacked_area, building_dummies], axis=1)\n",
    "data_stacked_area.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stacked_area = data_stacked_area.rename(columns={'1':'building1','2':'building2','3':'building3','4':'building4','5':'building5'})\n",
    "data_stacked_area.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, let's filter out only the sold apartments, as they are the only ones of interest.\n",
    "data_stacked_area = data_stacked_area[data_stacked_area['sold'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_of_interest = ['year_sale','building1','building2','building3','building4','building5']\n",
    "stacked_area = data_stacked_area[columns_of_interest].groupby('year_sale').sum()\n",
    "stacked_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"#264653\", \"#2A9D8F\", \"E9C46A\",'F4A261','E76F51']\n",
    "labels = ['Building 1','Building 2','Building 3','Building 4','Building 5',]\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize = (12, 6))\n",
    "\n",
    "plt.stackplot(stacked_area.index, stacked_area['building1'],stacked_area['building2'],stacked_area['building3'],stacked_area['building4'],\n",
    "              stacked_area['building5'],colors = colors,edgecolor = 'none')\n",
    "\n",
    "plt.xticks(stacked_area.index, rotation = 45) \n",
    "plt.legend(labels = labels, loc = \"upper left\") \n",
    "plt.ylabel(\"Number of Sales\", fontsize = 13)\n",
    "plt.xticks(fontsize = 13)\n",
    "plt.yticks(fontsize = 13)\n",
    "\n",
    "plt.title(\"Total Number of Sales per Year by Building\", fontsize = 18)\n",
    "sns.despine()\n",
    "plt.savefig(\"total_sales_per_year_per_building_stacked_area_chart.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_area_adj = stacked_area.copy() \n",
    "stacked_area_adj = stacked_area_adj.append({'building1': 0, \n",
    "                                     \"building2\": 0, \n",
    "                                     \"building3\": 0,\n",
    "                                     \"building4\": 0,\n",
    "                                     \"building5\": 0},ignore_index=True)\n",
    "\n",
    "# Adjust the index to include 2009.\n",
    "stacked_area_adj.index = ['2004','2005','2006','2007','2008','2010','2009']\n",
    "stacked_area_adj = stacked_area_adj.loc[['2004','2005','2006','2007','2008','2009','2010']]\n",
    "\n",
    "######################################################################################################\n",
    "\n",
    "colors = [\"#264653\", \"#2A9D8F\", \"E9C46A\",'F4A261','E76F51']\n",
    "labels = ['Building 1','Building 2','Building 3','Building 4','Building 5',]\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize = (12, 6))\n",
    "\n",
    "plt.stackplot(stacked_area_adj.index, stacked_area_adj['building1'],stacked_area_adj['building2'],stacked_area_adj['building3'],\n",
    "              stacked_area_adj['building4'],stacked_area_adj['building5'],colors = colors,edgecolor = 'none')\n",
    "\n",
    "plt.xticks(stacked_area_adj.index, rotation = 45) \n",
    "plt.legend(labels = labels, loc = \"upper left\") \n",
    "plt.ylabel(\"Number of Sales\", fontsize = 13)\n",
    "plt.xticks(fontsize = 13) \n",
    "plt.yticks(fontsize = 13) \n",
    "\n",
    "plt.title(\"Total Number of Sales per Year by Building\", fontsize = 18)\n",
    "sns.despine()\n",
    "plt.savefig(\"total_sales_per_year_per_building_stacked_area_chart_v2.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
